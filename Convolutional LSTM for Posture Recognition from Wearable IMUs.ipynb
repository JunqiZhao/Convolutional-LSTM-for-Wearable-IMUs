{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks (DNN) Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from numpy import genfromtxt\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten, Reshape,Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timeit #package for recording the model running time\n",
    "import time\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, Conv3D, MaxPooling3D, Reshape, BatchNormalization, MaxPooling2D\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,ShuffleSplit,StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import  preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report,f1_score,accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win_seg(data,windowsize,overlap):#function for overlap segmentation\n",
    "    length=int((data.shape[0]*data.shape[1]-windowsize)/(windowsize*overlap)+1)\n",
    "    newdata=np.empty((length,windowsize, data.shape[2],1))\n",
    "    data_dim=data.shape[2]\n",
    "    layers=data.shape[3]\n",
    "    data=data.reshape(-1,data_dim,layers)\n",
    "    for i in range(0,length) :\n",
    "        start=int(i*windowsize*overlap)\n",
    "        end=int(start+windowsize)\n",
    "        newdata[i]=data[start:end]\n",
    "    return newdata\n",
    "def lab_vote(data,windowsize):\n",
    "    y_data=data.reshape(-1,windowsize,1,1)\n",
    "    y_data=win_seg(y_data,windowsize,0.5)\n",
    "    y_data=y_data.reshape(y_data.shape[0],y_data.shape[1],y_data.shape[2])\n",
    "    y_data=stats.mode(y_data,axis=1)\n",
    "    y_data=y_data.mode\n",
    "    y_data=y_data.reshape(-1,1)\n",
    "    y_data=np.float64(keras.utils.to_categorical(y_data))\n",
    "    return y_data\n",
    "def lab_vote_cat(data,windowsize): # non one-hot coding\n",
    "    y_data=data.reshape(-1,windowsize,1,1)\n",
    "    y_data=win_seg(y_data,windowsize,0.5)\n",
    "    y_data=y_data.reshape(y_data.shape[0],y_data.shape[1],y_data.shape[2])\n",
    "    y_data=stats.mode(y_data,axis=1)\n",
    "    y_data=y_data.mode\n",
    "    y_data=y_data.reshape(-1,1)\n",
    "    return y_data\n",
    "def write_csv(data):\n",
    "    a = np.asarray(data)\n",
    "    a.tofile('check.csv',sep=',',format='%10.5f')\n",
    "def average(lst): \n",
    "    a = np.array(lst)\n",
    "    return np.mean(a)\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional LSTM Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training and testing data\n",
    "os.chdir(\"...\") #changing working directory\n",
    "buffer = np.float64(preprocessing.scale(genfromtxt('S3_X.csv', delimiter=','))) # using S3 as an example\n",
    "x_data=buffer.reshape(-1,40,30,1)\n",
    "x_data=win_seg(x_data,40,0.5) # data segmentation with 0.5 overlap\n",
    "#majority vote on training label\n",
    "buffer = np.float64(genfromtxt('S3_Y.csv', delimiter=','))-1 #0 based index\n",
    "y_data=lab_vote(buffer,40)\n",
    "y_data2=lab_vote_cat(buffer,40) # for stratification purposes\n",
    "#five round Stratified Random Shuffle\n",
    "SRS=StratifiedShuffleSplit(n_splits=5, test_size=0.1, random_state=42) #split the train and test by 9:1\n",
    "#model evaluation metrics\n",
    "acc_score=list()\n",
    "f_score=list()\n",
    "eopch_time_record=list()\n",
    "oper_time_record=list()\n",
    "i=0\n",
    "for train_index, test_index in SRS.split(x_data,y_data):\n",
    "    X_train, X_test = x_data[train_index], x_data[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "    #split the train data into training (training the model) and validation (tuning hypeparameters) by 8:2\n",
    "    X_training, X_validation, y_training, y_validation = train_test_split(X_train, y_train, test_size=0.20)\n",
    "    #setup model parameters\n",
    "    data_dim = X_train.shape[2] #y of 2D Motion Image\n",
    "    timesteps = X_train.shape[1] #x of 2D Motion Image\n",
    "    num_classes = y_train.shape[1]\n",
    "    batchsize=300\n",
    "    epcoh=300\n",
    "    #build model\n",
    "    model = Sequential()\n",
    "    #five convolutional layers as an exmaple, adjust the convolutional layer depth if needed\n",
    "    model.add(Conv2D(64, kernel_size=(5, 30), strides=(1, 1),padding='same',\n",
    "                 activation='tanh',input_shape=(timesteps, data_dim,1)))\n",
    "    model.add(Conv2D(64, kernel_size=(5, 30), strides=(1, 1),padding='same',\n",
    "                 activation='tanh'))\n",
    "    model.add(Conv2D(64, kernel_size=(5, 30), strides=(1, 1),padding='same',\n",
    "                 activation='tanh'))\n",
    "    model.add(Conv2D(64, kernel_size=(5, 30), strides=(1, 1),padding='same',\n",
    "                 activation='tanh'))\n",
    "    model.add(Conv2D(64, kernel_size=(5, 30), strides=(1, 1),padding='same',\n",
    "                 activation='tanh'))\n",
    "    #turn the multilayer tensor into single layer tensor\n",
    "    model.add(Reshape((40, -1),input_shape=(40,30,64)))\n",
    "    model.add(Dropout(0.5)) #add dropout layers for controlling overfitting\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(40, 1920)))  # returns a sequence of vectors\n",
    "    model.add(Dropout(0.5)) #add dropout layers for controlling overfitting\n",
    "    model.add(LSTM(128))  # return a single vector\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),metrics=['accuracy',f1])\n",
    "    checkpointer = ModelCheckpoint(filepath=\"2D_CNN5_LSTM_checkpoint(F1)_sss_%s.h5\" % i, monitor='val_f1',verbose=1, mode='max', save_best_only=True)\n",
    "    time_callback = TimeHistory() #record the model training time for each epoch\n",
    "    callbacks_list = [checkpointer,time_callback] \n",
    "    train_history=model.fit(X_training, y_training,\n",
    "              batch_size=batchsize, epochs=epcoh,callbacks=callbacks_list,\n",
    "              validation_data=(X_validation, y_validation))\n",
    "    eopch_time=time_callback.times\n",
    "    eopch_time_record.append(eopch_time) #record the traing time of each epoch\n",
    "    CNN_LSTM_model=load_model(\"2D_CNN5_LSTM_checkpoint(F1)_sss_%s.h5\" % i, custom_objects={'f1': f1})\n",
    "    #model operation and timing\n",
    "    start=timeit.default_timer()\n",
    "    y_pred=CNN_LSTM_model.predict(X_test)\n",
    "    stop=timeit.default_timer()\n",
    "    oper_time=stop-start\n",
    "    oper_time_record.append(oper_time)\n",
    "    #check the model test result\n",
    "    y_pred=CNN_LSTM_model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    Y_test=np.argmax(y_test, axis=1)\n",
    "    acc_score.append(accuracy_score(Y_test, y_pred)) # Evaluation of accuracy\n",
    "    f_score.append(f1_score(Y_test, y_pred,average='macro')) # Evaluation of F1 score\n",
    "    print(\"This is the\", i+1,  \"out of \",5, \"Shuffle\")\n",
    "    i+=1\n",
    "    del model #delete the model for retrain the neural network from scrach, instead of starting from trained model\n",
    "    \n",
    "# record performance\n",
    "performance=pd.DataFrame(columns=['Acc_score','Macro_Fscore','Average_Epoch','Average_Run'])\n",
    "performance['Acc_score']=acc_score\n",
    "performance['Macro_Fscore']=f_score\n",
    "performance['Average_Epoch']=average(eopch_time_record)\n",
    "performance['Average_Run']=average(oper_time_record)\n",
    "performance.to_csv(\"2DConv5LSTM_Performance_sss_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline LSTM Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score=list()\n",
    "f_score=list()\n",
    "eopch_time_record=list()\n",
    "oper_time_record=list()\n",
    "#loading data\n",
    "buffer = np.float64(preprocessing.scale(genfromtxt('S3_X.csv', delimiter=',')))\n",
    "x_data=buffer.reshape(-1,40,30,1)\n",
    "x_data=win_seg(x_data,40,0.5) # data segmentation with 0.5 overlap\n",
    "x_data=x_data.reshape(x_data.shape[0],x_data.shape[1],x_data.shape[2]) #reshape the dataset as LSTM input shape\n",
    "#majority vote on training label\n",
    "buffer = np.float64(genfromtxt('S3_Y.csv', delimiter=','))-1 #0 based index\n",
    "y_data=lab_vote(buffer,40)\n",
    "i=0\n",
    "for train_index, test_index in SRS.split(x_data,y_data):\n",
    "    X_train, X_test = x_data[train_index], x_data[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "    #split the train data into training (training the model) and validation (tuning hypeparameters) by 8:2\n",
    "    X_training, X_validation, y_training, y_validation = train_test_split(X_train, y_train, test_size=0.20)\n",
    "    #setup model parameters\n",
    "    data_dim = X_train.shape[2] #y of figure\n",
    "    timesteps = X_train.shape[1] #x of figure\n",
    "    num_classes = y_train.shape[1]\n",
    "    batchsize=300\n",
    "    epcoh=300\n",
    "    #Build Model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=(timesteps, data_dim)))   # returns a sequence of vectors of dimension 64\n",
    "    model.add(Dropout(0.5)) #add dropout layers for controlling overfitting\n",
    "    model.add(LSTM(128))  # return a single vector of dimension 64\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),metrics=['accuracy',f1])\n",
    "    checkpointer = ModelCheckpoint(filepath='LSTM_checkpoint(F1)_sss_%s.h5' % i, monitor='val_f1',verbose=1,mode='max', save_best_only=True)\n",
    "    time_callback = TimeHistory() #record the model training time for each epoch\n",
    "    callbacks_list = [checkpointer,time_callback] \n",
    "    model.fit(X_training, y_training,\n",
    "              batch_size=batchsize, epochs=epcoh,callbacks=callbacks_list,\n",
    "              validation_data=(X_validation, y_validation))\n",
    "    eopch_time=time_callback.times\n",
    "    eopch_time_record.append(eopch_time) #record the traing time of each epoch\n",
    "    LSTM_model=load_model('LSTM_checkpoint(F1)_sss_%s.h5' % i,custom_objects={'f1': f1})\n",
    "    #model operation and timing\n",
    "    start=timeit.default_timer()\n",
    "    y_pred=LSTM_model.predict(X_test)\n",
    "    stop=timeit.default_timer()\n",
    "    oper_time=stop-start\n",
    "    oper_time_record.append(oper_time)\n",
    "    #check the model test result\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    Y_test=np.argmax(y_test, axis=1)\n",
    "    acc_score.append(accuracy_score(Y_test, y_pred))\n",
    "    f_score.append(f1_score(Y_test, y_pred,average='macro'))\n",
    "    print(\"This is the\", i+1,  \"out of \",5, \"Shuffle\")\n",
    "    del model #delete the model for retrain the neural network from scrach, instead of starting from trained model\n",
    "    i+=1\n",
    "# record performance\n",
    "performance=pd.DataFrame(columns=['Acc_score','Macro_Fscore','Average_Epoch','Average_Run'])\n",
    "performance['Acc_score']=acc_score\n",
    "performance['Macro_Fscore']=f_score\n",
    "performance['Average_Epoch']=average(eopch_time_record)\n",
    "performance['Average_Run']=average(oper_time_record)\n",
    "performance.to_csv(\"LSTM_Performance_sss_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score=list()\n",
    "f_score=list()\n",
    "eopch_time_record=list()\n",
    "oper_time_record=list()\n",
    "i=0\n",
    "for train_index, test_index in SRS.split(x_data,y_data):\n",
    "    X_train, X_test = x_data[train_index], x_data[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "    #split the train data into training (training the model) and validation (tuning hypeparameters) by 8:2\n",
    "    X_training, X_validation, y_training, y_validation = train_test_split(X_train, y_train, test_size=0.20)\n",
    "    #setup model parameters\n",
    "    data_dim = X_train.shape[2] #y of figure\n",
    "    timesteps = X_train.shape[1] #x of figure\n",
    "    num_classes = y_train.shape[1]\n",
    "    batchsize=300\n",
    "    epcoh=300\n",
    "    #Build Model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(5, 30), strides=(1, 1),padding='same',\n",
    "                 activation='tanh',input_shape=(timesteps, data_dim,1)))\n",
    "    model.add(Conv2D(64, kernel_size=(5, 30), strides=(1, 1),padding='same',\n",
    "                 activation='tanh'))\n",
    "    model.add(Conv2D(64, kernel_size=(5, 30), strides=(1, 1),padding='same',\n",
    "                 activation='tanh'))\n",
    "    model.add(Conv2D(64, kernel_size=(5, 30), strides=(1, 1),padding='same',\n",
    "                 activation='tanh'))\n",
    "    model.add(Conv2D(64, kernel_size=(5, 30), strides=(1, 1),padding='same',\n",
    "                 activation='tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5)) #add dropout layers for controlling overfitting\n",
    "    model.add(Dense(128, activation='tanh'))\n",
    "    model.add(Dropout(0.5)) #add dropout layers for controlling overfitting\n",
    "    model.add(Dense(128, activation='tanh'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))#second flat fully connected layer for softmatrix (classification)\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),metrics=['accuracy',f1])\n",
    "    checkpointer = ModelCheckpoint(filepath='2D_CNN_checkpoint(F1)_sss_%s.h5' % i, monitor='val_f1',mode='max',verbose=1, save_best_only=True)\n",
    "    time_callback = TimeHistory() #record the model training time for each epoch\n",
    "    callbacks_list = [checkpointer,time_callback] \n",
    "    model.fit(X_training, y_training,\n",
    "              batch_size=batchsize, epochs=epcoh,callbacks=callbacks_list,\n",
    "              validation_data=(X_validation, y_validation))\n",
    "    eopch_time=time_callback.times\n",
    "    eopch_time_record.append(eopch_time) #record the traingtime of each epoch\n",
    "    CNN_model=load_model('2D_CNN_checkpoint(F1)_sss_%s.h5' % i, custom_objects={'f1': f1})\n",
    "    #model operation and timing\n",
    "    start=timeit.default_timer()\n",
    "    y_pred=CNN_model.predict(X_test)\n",
    "    stop=timeit.default_timer()\n",
    "    oper_time=stop-start\n",
    "    oper_time_record.append(oper_time)\n",
    "    #check the model test result\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    Y_test=np.argmax(y_test, axis=1)\n",
    "    acc_score.append(accuracy_score(Y_test, y_pred))\n",
    "    f_score.append(f1_score(Y_test, y_pred,average='macro'))\n",
    "    print(\"This is the\", i+1,  \"out of \",5, \"Shuffle\")\n",
    "    del model #delete the model for retrain the neural network from scrach, instead of starting from trained model\n",
    "    i+=1\n",
    "    \n",
    "# record performance\n",
    "import pandas as pd\n",
    "performance=pd.DataFrame(columns=['Acc_score','Macro_Fscore','Average_Epoch','Average_Run'])\n",
    "performance['Acc_score']=acc_score\n",
    "performance['Macro_Fscore']=f_score\n",
    "performance['Average_Epoch']=average(eopch_time_record)\n",
    "performance['Average_Run']=average(oper_time_record)\n",
    "performance.to_csv(\"2DConv_Performance_sss_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Machine Learing-based Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import  preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report,f1_score,accuracy_score\n",
    "import timeit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win_seg(data,windowsize,overlap):#function for overlap segmentation\n",
    "    length=int((data.shape[0]*data.shape[1]-windowsize)/(windowsize*overlap)+1)\n",
    "    newdata=np.empty((length,windowsize, data.shape[2],1))\n",
    "    data_dim=data.shape[2]\n",
    "    layers=data.shape[3]\n",
    "    data=data.reshape(-1,data_dim,layers)\n",
    "    for i in range(0,length) :\n",
    "        start=int(i*windowsize*overlap)\n",
    "        end=int(start+windowsize)\n",
    "        newdata[i]=data[start:end]\n",
    "    return newdata\n",
    "def lab_vote(data,windowsize):\n",
    "    y_data=data.reshape(-1,windowsize,1,1)\n",
    "    y_data=win_seg(y_data,windowsize,0.5)\n",
    "    y_data=y_data.reshape(y_data.shape[0],y_data.shape[1],y_data.shape[2])\n",
    "    y_data=stats.mode(y_data,axis=1)\n",
    "    y_data=y_data.mode\n",
    "    y_data=y_data.reshape(-1,1)\n",
    "    y_data=np.float64(keras.utils.to_categorical(y_data))\n",
    "    return y_data\n",
    "def lab_vote_cat(data,windowsize): # non one-hot coding\n",
    "    y_data=data.reshape(-1,windowsize,1,1)\n",
    "    y_data=win_seg(y_data,windowsize,0.5)\n",
    "    y_data=y_data.reshape(y_data.shape[0],y_data.shape[1],y_data.shape[2])\n",
    "    y_data=stats.mode(y_data,axis=1)\n",
    "    y_data=y_data.mode\n",
    "    y_data=y_data.reshape(-1,1)\n",
    "    return y_data\n",
    "def preparation(dataset):\n",
    "    x_data=preprocessing.scale(pd.read_csv(dataset).iloc[:,1:]) #Column-wise normalization\n",
    "    y_data=pd.read_csv(dataset).iloc[:,0]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.20, random_state=42)#split the data into train and test by 8:2\n",
    "    return X_train, X_test, x_data,y_train, y_test, y_data\n",
    "def TrainModels(X_train, X_test, y_train, y_test):\n",
    "    # Time cost\n",
    "    train_time=[]\n",
    "    run_time=[]\n",
    "    #SVM\n",
    "    svm=SVC(gamma='auto',random_state=42)\n",
    "    start = timeit.default_timer()\n",
    "    svm.fit(X_train,y_train)\n",
    "    stop = timeit.default_timer()\n",
    "    train_time.append(stop - start)\n",
    "    start = timeit.default_timer()\n",
    "    svm_pre=pd.DataFrame(data=svm.predict(X_test))\n",
    "    stop = timeit.default_timer()\n",
    "    run_time.append(stop - start)\n",
    "    #Naive Bayes\n",
    "    nb=GaussianNB()\n",
    "    start = timeit.default_timer()\n",
    "    nb.fit(X_train,y_train)\n",
    "    stop = timeit.default_timer()\n",
    "    train_time.append(stop - start)\n",
    "    start = timeit.default_timer()\n",
    "    nb_pre=pd.DataFrame(data=nb.predict(X_test))\n",
    "    stop = timeit.default_timer()\n",
    "    run_time.append(stop - start)\n",
    "    #KNN\n",
    "    knn=KNeighborsClassifier(n_neighbors=7) # based on a simple grid search\n",
    "    start = timeit.default_timer()\n",
    "    knn.fit(X_train,y_train)\n",
    "    stop = timeit.default_timer()\n",
    "    train_time.append(stop - start)\n",
    "    start = timeit.default_timer()\n",
    "    knn_pre=pd.DataFrame(data=knn.predict(X_test))\n",
    "    stop = timeit.default_timer()\n",
    "    run_time.append(stop - start)\n",
    "    #Decision Tree\n",
    "    dt=DecisionTreeClassifier(random_state=42)\n",
    "    start = timeit.default_timer()\n",
    "    dt.fit(X_train,y_train)\n",
    "    stop = timeit.default_timer()\n",
    "    train_time.append(stop - start)\n",
    "    start = timeit.default_timer()\n",
    "    dt_pre= pd.DataFrame(data=dt.predict(X_test))\n",
    "    stop = timeit.default_timer()\n",
    "    run_time.append(stop - start)\n",
    "    #Random Forest\n",
    "    rf=RandomForestClassifier(n_estimators=100)\n",
    "    start = timeit.default_timer()\n",
    "    rf.fit(X_train,y_train)\n",
    "    stop = timeit.default_timer()\n",
    "    train_time.append(stop - start)\n",
    "    start = timeit.default_timer()\n",
    "    rf_pre=pd.DataFrame(data=rf.predict(X_test))\n",
    "    stop = timeit.default_timer()\n",
    "    run_time.append(stop - start)\n",
    "    report = pd.DataFrame(columns=['Models','Accuracy','Macro F1','Micro F1','Train Time','Run Time'])\n",
    "    report['Models']=modelnames\n",
    "    for i in range(len(result.columns)):\n",
    "        report.iloc[i,1]=accuracy_score(y_test, result.iloc[:,i])\n",
    "        report.iloc[i,2]=f1_score(y_test, result.iloc[:,i],average='macro')\n",
    "        report.iloc[i,3]=f1_score(y_test, result.iloc[:,i],average='micro')\n",
    "        if i<len(train_time):\n",
    "            report.iloc[i,4]=train_time[i]\n",
    "            report.iloc[i,5]=run_time[i]\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training and testing data\n",
    "os.chdir(\"...\") #changing working directory\n",
    "buffer = np.float64(genfromtxt('S3_X.csv', delimiter=','))\n",
    "x_data=buffer.reshape(-1,40,30,1)\n",
    "x_data=win_seg(x_data,40,0.5) # data segmentation with 0.5 overlap\n",
    "x_data=x_data.reshape(-1,40,30)\n",
    "x_data_pd=x_data.reshape(-1,30)\n",
    "x_data_pd = pd.DataFrame(data=x_data_pd)\n",
    "adj_win=[i//40+1 for i in range(len(x_data_pd.iloc[:,0]))]\n",
    "x_data_pd[\"adjwin\"]=adj_win\n",
    "x_data_pd.to_csv(\"S3_X_ML.csv\")\n",
    "#majority vote on training label\n",
    "buffer = np.float64(genfromtxt('S3_Y.csv', delimiter=',')) #0 based index\n",
    "y_data=lab_vote(buffer,40)\n",
    "y_data2=lab_vote_cat(buffer,40) # for stratification purposes\n",
    "y_data_pd = pd.DataFrame(data=y_data2)\n",
    "y_data_pd.to_csv(\"S3_Y_ML.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Using Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X_train, y_train\n",
    "svc = SVC(kernel=\"linear\")\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(10),scoring='f1_macro')\n",
    "rfecv.fit(X, y)\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "#plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()\n",
    "# Export the best features\n",
    "sel_features=pd.DataFrame()\n",
    "sel_features[\"label\"]=y_test\n",
    "fullfeatures=pd.read_csv(\"fullfeatures.csv\")\n",
    "names=list(fullfeatures.columns.values)[1:]\n",
    "for index, val in enumerate(list(rfecv.support_)):\n",
    "    if val:\n",
    "        sel_features=pd.concat([sel_features,fullfeatures.iloc[:,index+1]],axis=1)\n",
    "sel_features.to_csv(\"S3_Dataset_ML_SelectetedFeatures.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_data,y_train, y_test, y_data=preparation(\"S3_Dataset_ML_SelectetedFeatures.csv\")\n",
    "sf = ShuffleSplit(n_splits=5, test_size=0.1, random_state=42) # Random Shuffle\n",
    "SRS = StratifiedShuffleSplit(n_splits=5, test_size=0.1, random_state=42) # Stratified Shuffle\n",
    "finalreport = pd.DataFrame(columns=['Models','Accuracy','Macro F1','Micro F1','Train Time','Run Time'])\n",
    "for train_index, test_index in SRS.split(X_data, y_data):\n",
    "    X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "    finalreport=finalreport.append(TrainModels(X_train, X_test, y_train, y_test))\n",
    "finalreport.to_csv(\"S3_Dataset_ML_SelectetedFeatures_Evalucation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
