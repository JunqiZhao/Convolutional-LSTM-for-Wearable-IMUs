{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
    "from sklearn import  preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report,f1_score,accuracy_score\n",
    "import timeit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,ShuffleSplit,StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win_seg(data,windowsize,overlap):#function for overlap segmentation\n",
    "    length=int((data.shape[0]*data.shape[1]-windowsize)/(windowsize*overlap)+1)\n",
    "    newdata=np.empty((length,windowsize, data.shape[2],1))\n",
    "    data_dim=data.shape[2]\n",
    "    layers=data.shape[3]\n",
    "    data=data.reshape(-1,data_dim,layers)\n",
    "    for i in range(0,length) :\n",
    "        start=int(i*windowsize*overlap)\n",
    "        end=int(start+windowsize)\n",
    "        newdata[i]=data[start:end]\n",
    "    return newdata\n",
    "def lab_vote(data,windowsize):\n",
    "    y_data=data.reshape(-1,windowsize,1,1)\n",
    "    y_data=win_seg(y_data,windowsize,0.5)\n",
    "    y_data=y_data.reshape(y_data.shape[0],y_data.shape[1],y_data.shape[2])\n",
    "    y_data=stats.mode(y_data,axis=1)\n",
    "    y_data=y_data.mode\n",
    "    y_data=y_data.reshape(-1,1)\n",
    "    y_data=np.float64(keras.utils.to_categorical(y_data))\n",
    "    return y_data\n",
    "def lab_vote_cat(data,windowsize): # non one-hot coding\n",
    "    y_data=data.reshape(-1,windowsize,1,1)\n",
    "    y_data=win_seg(y_data,windowsize,0.5)\n",
    "    y_data=y_data.reshape(y_data.shape[0],y_data.shape[1],y_data.shape[2])\n",
    "    y_data=stats.mode(y_data,axis=1)\n",
    "    y_data=y_data.mode\n",
    "    y_data=y_data.reshape(-1,1)\n",
    "    return y_data\n",
    "def preparation(dataset):\n",
    "    x_data=preprocessing.scale(pd.read_csv(dataset).iloc[:,1:]) #Column-wise normalization\n",
    "    y_data=pd.read_csv(dataset).iloc[:,0]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.20, random_state=42)#split the data into train and test by 8:2\n",
    "    return X_train, X_test, x_data,y_train, y_test, y_data\n",
    "def TrainModels_Tuned(X_train, X_test, y_train, y_test,\n",
    "                      svm_c,\n",
    "                      svm_gamma,\n",
    "                      svm_kernel,\n",
    "                      svm_degree,\n",
    "                      nb_var,\n",
    "                      knn_k,\n",
    "                      dt_depth,\n",
    "                      dt_split,\n",
    "                      dt_leaf,\n",
    "                      rf_trees\n",
    "                     ):\n",
    "    # Time cost\n",
    "    train_time=[]\n",
    "    run_time=[]\n",
    "    #SVM\n",
    "    svm=SVC(gamma=svm_gamma, C=svm_c, kernel=svm_kernel, degree=svm_degree, random_state=42)\n",
    "    start = timeit.default_timer()\n",
    "    svm.fit(X_train,y_train)\n",
    "    stop = timeit.default_timer()\n",
    "    train_time.append(stop - start)\n",
    "    start = timeit.default_timer()\n",
    "    svm_pre=pd.DataFrame(data=svm.predict(X_test))\n",
    "    stop = timeit.default_timer()\n",
    "    run_time.append(stop - start)\n",
    "    #Naive Bayes\n",
    "    nb=GaussianNB(var_smoothing=nb_var)\n",
    "    start = timeit.default_timer()\n",
    "    nb.fit(X_train,y_train)\n",
    "    stop = timeit.default_timer()\n",
    "    train_time.append(stop - start)\n",
    "    start = timeit.default_timer()\n",
    "    nb_pre=pd.DataFrame(data=nb.predict(X_test))\n",
    "    stop = timeit.default_timer()\n",
    "    run_time.append(stop - start)\n",
    "    #KNN\n",
    "    knn=KNeighborsClassifier(n_neighbors=knn_k) # based on a simple grid search\n",
    "    start = timeit.default_timer()\n",
    "    knn.fit(X_train,y_train)\n",
    "    stop = timeit.default_timer()\n",
    "    train_time.append(stop - start)\n",
    "    start = timeit.default_timer()\n",
    "    knn_pre=pd.DataFrame(data=knn.predict(X_test))\n",
    "    stop = timeit.default_timer()\n",
    "    run_time.append(stop - start)\n",
    "    \n",
    "    #Decision Tree\n",
    "    dt=dt=DecisionTreeClassifier(max_depth=dt_depth, min_samples_leaf=dt_leaf, min_samples_split=dt_split,random_state=42)\n",
    "    start = timeit.default_timer()\n",
    "    dt.fit(X_train,y_train)\n",
    "    stop = timeit.default_timer()\n",
    "    train_time.append(stop - start)\n",
    "    start = timeit.default_timer()\n",
    "    dt_pre= pd.DataFrame(data=dt.predict(X_test))\n",
    "    stop = timeit.default_timer()\n",
    "    run_time.append(stop - start)\n",
    "    \n",
    "    #Random Forest\n",
    "    rf=RandomForestClassifier(n_estimators=rf_trees)\n",
    "    start = timeit.default_timer()\n",
    "    rf.fit(X_train,y_train)\n",
    "    stop = timeit.default_timer()\n",
    "    train_time.append(stop - start)\n",
    "    start = timeit.default_timer()\n",
    "    rf_pre=pd.DataFrame(data=rf.predict(X_test))\n",
    "    stop = timeit.default_timer()\n",
    "    run_time.append(stop - start)\n",
    "    \n",
    "    #Ensemble\n",
    "    result=pd.concat([svm_pre,nb_pre, knn_pre,dt_pre,rf_pre], axis=1)\n",
    "    result.columns=['svm_pre','nb_pre', 'knn_pre','dt_pre','rf_pre']\n",
    "    result['Ensemble']=result.mode(axis='columns').iloc[:,0]\n",
    "    modelnames=list(result.columns)\n",
    "    #classnames=list(set(y_test))\n",
    "    #report=classification_report(y_test, svm_pre, target_names=classnames) #general report\n",
    "    report = pd.DataFrame(columns=['Models','Accuracy','Macro F1','Micro F1','Train Time','Run Time'])\n",
    "    report['Models']=modelnames\n",
    "    for i in range(len(result.columns)):\n",
    "        report.iloc[i,1]=accuracy_score(y_test, result.iloc[:,i])\n",
    "        report.iloc[i,2]=f1_score(y_test, result.iloc[:,i],average='macro')\n",
    "        report.iloc[i,3]=f1_score(y_test, result.iloc[:,i],average='micro')\n",
    "        if i<len(train_time):\n",
    "            report.iloc[i,4]=train_time[i]\n",
    "            report.iloc[i,5]=run_time[i]\n",
    "    return report\n",
    "\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [2**i for i in [-10, -5, -1, 0, 1, 5, 10]]\n",
    "    gammas =[2**i for i in [-10, -5, -1, 0, 1, 5, 10]]\n",
    "    kernels = ['poly', 'rbf']\n",
    "    degrees = [2, 3, 4, 5]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas, 'kernel':kernels, 'degree': degrees}\n",
    "    com=1\n",
    "    for x in param_grid.values():\n",
    "        com *= len(x)\n",
    "    print('There are {} combinations'.format(com))\n",
    "    grid_search = GridSearchCV(SVC(), param_grid, cv=nfolds, scoring='f1_macro')\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_\n",
    "def NB_param_selection(X, y, nfolds):\n",
    "    param_grid = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "    com=1\n",
    "    for x in param_grid.values():\n",
    "        com *= len(x)\n",
    "    print('There are {} combinations'.format(com))\n",
    "    grid_search = GridSearchCV(GaussianNB(), param_grid, cv=nfolds, scoring='f1_macro')\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_\n",
    "def KNN_param_selection(X, y, nfolds):\n",
    "    K = [i+1 for i in range(100)]\n",
    "    param_grid = {'n_neighbors': K}\n",
    "    com=1\n",
    "    for x in param_grid.values():\n",
    "        com *= len(x)\n",
    "    print('There are {} combinations'.format(com))\n",
    "    grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=nfolds, scoring='f1_macro')\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_\n",
    "def DT_param_selection(X, y, nfolds):\n",
    "    max_depths = [i*5 for i in range(1,7)]\n",
    "    min_samples_splits = [i*5 for i in range(1,7)]\n",
    "    min_samples_leafs = [i*5 for i in range(1,7)]\n",
    "    param_grid = {#'ccp_alpha': ccp_alphas,\n",
    "                  'max_depth': max_depths,\n",
    "                  'min_samples_split': min_samples_splits,\n",
    "                  'min_samples_leaf': min_samples_leafs}\n",
    "    com=1\n",
    "    for x in param_grid.values():\n",
    "        com *= len(x)\n",
    "    print('There are {} combinations'.format(com))\n",
    "    grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=nfolds, scoring='f1_macro')\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_\n",
    "def RF_param_selection(X, y, nfolds):\n",
    "    n = [i*5 for i in range(1,81)]\n",
    "    param_grid = {\n",
    "    'n_estimators': n\n",
    "    }\n",
    "    com=1\n",
    "    for x in param_grid.values():\n",
    "        com *= len(x)\n",
    "    print('There are {} combinations'.format(com))\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=nfolds, scoring='f1_macro')\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"...\") #changing working directory\n",
    "X_train, X_test, X_data,y_train, y_test, y_data=preparation(\"...\")\n",
    "SRS = StratifiedShuffleSplit(n_splits=5, test_size=0.1, random_state=42) # Stratified Shuffle\n",
    "SS_val= ShuffleSplit(n_splits=1, test_size=0.2, random_state=24) # random sample\n",
    "finalreport = pd.DataFrame(columns=['Models','Accuracy','Macro F1','Micro F1','Train Time','Run Time'])\n",
    "svm_parameters=[]\n",
    "nb_parameters=[]\n",
    "knn_parameters=[]\n",
    "dt_parameters=[]\n",
    "rf_parameters=[]\n",
    "for train_index, test_index in SRS.split(X_data, y_data):\n",
    "    # train/test spit\n",
    "    X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "    # fine tuning on validate and save the parameters & save parameters\n",
    "    svm_tuned = svc_param_selection(X_train, y_train, SS_val)\n",
    "    nb_tuned = NB_param_selection(X_train, y_train, SS_val)\n",
    "    knn_tuned = KNN_param_selection(X_train, y_train, SS_val)\n",
    "    dt_tuned = DT_param_selection(X_train, y_train, SS_val)\n",
    "    rf_tuned = RF_param_selection(X_train, y_train, SS_val)\n",
    "    svm_parameters.append(svm_tuned)\n",
    "    nb_parameters.append(nb_tuned)\n",
    "    knn_parameters.append(knn_tuned)\n",
    "    dt_parameters.append(dt_tuned)\n",
    "    rf_parameters.append(rf_tuned)\n",
    "    # get the configured model for test\n",
    "    finalreport=finalreport.append(TrainModels_Tuned(X_train, X_test, y_train, y_test,\n",
    "                      svm_c=svm_tuned['C'],\n",
    "                      svm_gamma=svm_tuned['gamma'],\n",
    "                      svm_kernel=svm_tuned['kernel'],\n",
    "                      svm_degree=svm_tuned['degree'],nb_var=nb_tuned['var_smoothing'],\n",
    "                      knn_k=knn_tuned['n_neighbors'],\n",
    "                      dt_depth=dt_tuned['max_depth'],\n",
    "                      dt_split=dt_tuned['min_samples_split'],\n",
    "                      dt_leaf=dt_tuned['min_samples_leaf'],\n",
    "                      rf_trees=rf_tuned['n_estimators']\n",
    "                     ))\n",
    "finalreport.to_csv(\"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
